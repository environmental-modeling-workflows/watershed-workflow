{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.155249,
     "end_time": "2022-03-07T15:48:59.904477",
     "exception": false,
     "start_time": "2022-03-07T15:48:59.749228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complete Workflow for generating ATS input for Coweeta\n",
    "\n",
    "This workflow provides a complete working example to develop an unstructured mesh for an integrated hydrologic model based on HUCs.  It is the default workflow for integrated hydrology simulations for Exasheds Simulation Campaign 2.\n",
    "\n",
    "It uses the following datasets:\n",
    "\n",
    "* `NHD Plus` for the watershed boundary and hydrography.\n",
    "* `NED` for elevation\n",
    "* `NLCD` for land cover/transpiration/rooting depths\n",
    "* `GLYHMPS` geology data for structural formations\n",
    "* `SoilGrids 2017` for depth to bedrock and soil texture information\n",
    "* `SSURGO` for soil data, where available, in the top 2m.\n",
    "\n",
    "Given some basic inputs (in the next cell) including a NAME, this workflow creates the following files (noting that some suffixes may be appended to the user-provided NAME in homogeneous cases):\n",
    "\n",
    "* Mesh file: `{NAME}.exo`, includes all labeled sets\n",
    "* Forcing: DayMet data -- daily raster of precip, RH, incoming radiation, etc.\n",
    "  - `{NAME}_DayMet_1910_2020.h5`, the DayMet data on this watershed\n",
    "  - `{NAME}_DayMet_typical_1910_2020.h5`, a \"typical year\" of DayMet, smoothed for spinup purposes, then looped 40 years\n",
    "* Forcing: LAI data -- every 4 days, time series by land cover type of LAI.  Note, the raw inputs to this are not done by NAME, but by an (optional, defaults to NAME) MODIS_NAME variable.  Since WW does not currently download MODIS, one might want to use a file of a different name to provide MODIS data.  The times of this MODIS data are hard-coded too -- this is all a bit wonky and will remain so until we get around to adding a file manager for MODIS data.\n",
    "  - `{NAME}_MODIS_LAI_smoothed_2010_2020.h5`, the LAI, interpolated and smoothed from the raw MODIS data\n",
    "  - `{NAME}_MODIS_LAI_typical_2010_2020.h5`, a \"typical year\" of LAI, smoothed for spinup purposes then looped 10 years\n",
    "* Input files: ATS xml files\n",
    "  - `spinup-steadystate-{NAME}.xml` the steady-state solution based on uniform application of mean rainfall rate\n",
    "  - `spinup-cyclic_steadystate-{NAME}.xml` the cyclic steady state based on typical years\n",
    "  - `transient-{NAME}.xml` the forward model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13880ecd-7fba-4d49-ac3a-dfd1d563c47a",
   "metadata": {
    "papermill": {
     "duration": 0.162792,
     "end_time": "2022-03-07T15:49:00.201712",
     "exception": false,
     "start_time": "2022-03-07T15:49:00.038920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these can be turned on for development work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4987b1f6-a1bc-4bf4-a97d-db0b3eff0c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/ecoon/code/miniforge3/envs/ww-geopandas-20250725/lib/python3.13/site-packages/netCDF4/_netCDF4.cpython-313-x86_64-linux-gnu.so: undefined symbol: nc_inq_var_quantize",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## FIX ME -- why is this broken without importing netcdf first?\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetCDF4\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/miniforge3/envs/ww-geopandas-20250725/lib/python3.13/site-packages/netCDF4/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# init for netCDF4. package\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Docstring comes from extension module _netCDF4.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_netCDF4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Need explicit imports for names beginning with underscores\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_netCDF4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[34m__doc__\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: /home/ecoon/code/miniforge3/envs/ww-geopandas-20250725/lib/python3.13/site-packages/netCDF4/_netCDF4.cpython-313-x86_64-linux-gnu.so: undefined symbol: nc_inq_var_quantize"
     ]
    }
   ],
   "source": [
    "## FIX ME -- why is this broken without importing netcdf first?\n",
    "import netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc581b68-0592-42aa-8bee-71a913ca7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets lost\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1705a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as pcm\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cftime, datetime\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.source_list\n",
    "import watershed_workflow.utils\n",
    "import watershed_workflow.plot\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.regions\n",
    "import watershed_workflow.daymet\n",
    "import watershed_workflow.land_cover_properties\n",
    "import watershed_workflow.resampling\n",
    "import watershed_workflow.condition\n",
    "import watershed_workflow.io\n",
    "import watershed_workflow.sources.standard_names as names\n",
    "\n",
    "import ats_input_spec\n",
    "import ats_input_spec.public\n",
    "import ats_input_spec.io\n",
    "\n",
    "import amanzi_xml.utils.io as aio\n",
    "import amanzi_xml.utils.search as asearch\n",
    "import amanzi_xml.utils.errors as aerrors\n",
    "\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9c839-7706-4428-8976-d90c9064afd0",
   "metadata": {},
   "source": [
    "## Parameters and data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403772e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'Coweeta'\n",
    "coweeta_shapefile = os.path.join('input_data', 'coweeta_basin.shp')\n",
    "hint = '0601'  # hint: HUC 4 containing this shape.  \n",
    "               # This is necessary to avoid downloading all HUCs to search for this shape\n",
    "\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)\n",
    "\n",
    "# Geomtric parameters tuning the degree of cleaning of the raw data and scales of hydrologic features to be considered\n",
    "simplify = 60 # length scale to target average edge\n",
    "ignore_small_rivers = 2 \n",
    "prune_by_area_fraction = 0.01 \n",
    "\n",
    "\n",
    "# simulation control\n",
    "start = datetime.date(2010,8,1)\n",
    "end = datetime.date(2011,8,1)\n",
    "min_porosity = 0.05 # minimum porosity considered too small\n",
    "max_permeability = 1.e-10 # max value allowed for permeability\n",
    "max_vg_alpha = 1.e-3 # max value of van Genuchten's alpha -- our correlation is not valid for some soils\n",
    "nyears_cyclic_steadystate = 4\n",
    "\n",
    "# triangle refinement control\n",
    "include_rivers = True\n",
    "# huc boundary refinement control\n",
    "refine_d0 = 20\n",
    "refine_d1 = 100\n",
    "refine_L0 = 70\n",
    "refine_L1 = 200\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n",
    "\n",
    "# soil structure\n",
    "use_geologic_layer = True\n",
    "\n",
    "# logistics\n",
    "generate_plots = True # plots take time to make and aren't always needed\n",
    "generate_daymet = True # potentially don't do Met data forcing\n",
    "generate_modis = True\n",
    "\n",
    "log_to_file = False  # if true, write to file instead of in the notebook output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39048e",
   "metadata": {
    "papermill": {
     "duration": 0.162783,
     "end_time": "2022-03-07T15:49:01.109301",
     "exception": false,
     "start_time": "2022-03-07T15:49:00.946518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter checking\n",
    "assert(simplify > 0 and simplify < 300)\n",
    "assert(ignore_small_rivers == None or (ignore_small_rivers >= 0 and ignore_small_rivers <= 100))\n",
    "assert(prune_by_area_fraction == None or (prune_by_area_fraction >= 0 and prune_by_area_fraction < 1))\n",
    "assert(start.year >= 1980 and end.year < 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695d127",
   "metadata": {
    "papermill": {
     "duration": 0.152343,
     "end_time": "2022-03-07T15:49:01.391199",
     "exception": false,
     "start_time": "2022-03-07T15:49:01.238856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a dictionary of outputs -- will include all filenames generated\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93821426",
   "metadata": {
    "papermill": {
     "duration": 0.169238,
     "end_time": "2022-03-07T15:49:05.337830",
     "exception": false,
     "start_time": "2022-03-07T15:49:05.168592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05763672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape and crs of the shape\n",
    "coweeta = watershed_workflow.getShapes(coweeta_shapefile, crs=crs)\n",
    "coweeta.rename(columns={'AREA' : names.AREA, 'LABEL' : names.NAME}, inplace=True)\n",
    "coweeta[names.ID] = coweeta.index.values\n",
    "coweeta.set_index(names.ID, inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2bdbc-1c95-4052-8070-4a21aeb47633",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('hello')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6569dcd",
   "metadata": {
    "papermill": {
     "duration": 0.152278,
     "end_time": "2022-03-07T15:49:05.936761",
     "exception": false,
     "start_time": "2022-03-07T15:49:05.784483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile. A wide range of data sources are available; here we use the defaults except for using NHD Plus for watershed boundaries and hydrography (the default is NHD, which is lower resolution and therefore smaller download sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17feb6e0",
   "metadata": {
    "papermill": {
     "duration": 0.181137,
     "end_time": "2022-03-07T15:49:06.850648",
     "exception": false,
     "start_time": "2022-03-07T15:49:06.669511",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "sources = watershed_workflow.source_list.getDefaultSources()\n",
    "sources['hydrography'] = watershed_workflow.source_list.hydrography_sources['NHDPlus HR']\n",
    "sources['HUC'] = watershed_workflow.source_list.huc_sources['WaterData WBD']\n",
    "\n",
    "#\n",
    "# This demo uses a few datasets that have been clipped out of larger, national\n",
    "# datasets and are distributed with the code.  This is simply to save download\n",
    "# time for this simple problem and to lower the barrier for trying out\n",
    "# Watershed Workflow.  A more typical workflow would delete these lines (as \n",
    "# these files would not exist for other watersheds).\n",
    "#\n",
    "# The default versions of these download large raster and shapefile files that\n",
    "# are defined over a very large region (globally or the entire US).\n",
    "#\n",
    "lc_file = ['input_data', 'land_cover', 'land_cover.tif']\n",
    "dtb_file = ['input_data', 'DTB', 'DTB.tif']\n",
    "geo_file = ['input_data', 'GLHYMPS', 'GLHYMPS.shp']\n",
    "if os.path.split(os.getcwd())[-1] == 'examples':\n",
    "    lc_file.insert(0, 'Coweeta')\n",
    "    dtb_file.insert(0, 'Coweeta')\n",
    "    geo_file.insert(0, 'Coweeta')\n",
    "\n",
    "sources['land cover'] = watershed_workflow.source_list.ManagerRaster(os.path.join(*lc_file))\n",
    "sources['geologic structure'] = watershed_workflow.source_list.ManagerGLHYMPS(os.path.join(*geo_file))\n",
    "\n",
    "#\n",
    "# the Pelletier DTB map is not particularly accurate at Coweeta -- the SoilGrids map seems to be better.  Here we will use a clipped version of that map\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.ManagerRaster(os.path.join(*dtb_file))\n",
    "watershed_workflow.source_list.logSources(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457ce61",
   "metadata": {},
   "source": [
    "## Sources and setup\n",
    "\n",
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed = watershed_workflow.split_hucs.SplitHUCs(coweeta)\n",
    "watershed.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a65bd",
   "metadata": {},
   "source": [
    "### Get Rivers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5828ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_rivers = True\n",
    "\n",
    "if include_rivers:  \n",
    "    # download/collect the river network within that shape's bounds\n",
    "    reaches = watershed_workflow.getShapesByGeometry(sources['hydrography'], watershed.exterior, crs, crs)\n",
    "    rivers = watershed_workflow.river_tree.createRivers(reaches, method='hydroseq')\n",
    "\n",
    "watershed_orig, rivers_orig = watershed, rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517566da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ws, rivs, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ws.plot(color='k', marker='+', markersize=10, ax=ax)\n",
    "    for river in rivs:\n",
    "        river.plot(marker='x', markersize=10, ax=ax)\n",
    "\n",
    "plot(watershed, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f525e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the originals for plotting comparisons\n",
    "def createCopy(watershed, rivers):\n",
    "    \"\"\"To compare before/after, we often want to create copies.  Note in real workflows most things are done in-place without copies.\"\"\"\n",
    "    return watershed.deepcopy(), [r.deepcopy() for r in rivers]\n",
    "    \n",
    "watershed, rivers = createCopy(watershed_orig, rivers_orig)\n",
    "\n",
    "# simplifying \n",
    "watershed_workflow.simplify(watershed, rivers, 75, 75, 100, 500)\n",
    "\n",
    "# greatly shrunk the rivers... shrink the dataframe too\n",
    "for river in rivers:\n",
    "    river.resetDataFrame()\n",
    "\n",
    "# Now that the river network is set, find the watershed boundary outlets\n",
    "for river in rivers:\n",
    "    watershed_workflow.hydrography.findOutletsByCrossings(watershed, river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(watershed, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc15fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should generate a zoomable map, showing different reaches and watersheds, \n",
    "# with discrete points.  Problem areas are clickable to get IDs for manual\n",
    "# modifications.\n",
    "m = watershed.explore(marker=False)\n",
    "for river in rivers_orig:\n",
    "    m = river.explore(m=m, column=None, color='black', name=river['name']+' raw', marker=False)\n",
    "for river in rivers:\n",
    "    m = river.explore(m=m)\n",
    "    \n",
    "# m = watershed_workflow.makeMap(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Triangulation\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "widths = dict({1:8,2:12,3:16})\n",
    "\n",
    "m2, areas, dists = watershed_workflow.tessalateRiverAligned(watershed,rivers, river_width=widths,\n",
    "                                              refine_min_angle=min_angle,refine_max_area=30000,\n",
    "                                              diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a raster for the elevation map, based on NED\n",
    "dem = sources['DEM'].getDataset(watershed.exterior.buffer(100), watershed.crs)\n",
    "\n",
    "# provide surface mesh elevations\n",
    "watershed_workflow.elevate(m2, dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the DEM raster\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot the DEM data\n",
    "im = dem.plot(ax=ax, cmap='terrain', add_colorbar=False)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('Elevation (m)', rotation=270, labelpad=15)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Digital Elevation Model (DEM)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "# Set equal aspect ratio\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756b890",
   "metadata": {},
   "source": [
    "In the pit-filling algorithm, we want to make sure that river corridor is not filled up. Hence we exclude river corridor cells from the pit-filling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits\n",
    "river_mask=np.zeros((len(m2.conn)))\n",
    "for i, elem in enumerate(m2.conn):\n",
    "    if not len(elem)==3:\n",
    "        river_mask[i]=1     \n",
    "watershed_workflow.condition.fillPitsDual(m2, is_waterbody=river_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cc9d6",
   "metadata": {},
   "source": [
    "There are a range of options to condition river corridor mesh. We hydrologically condition the river mesh, ensuring unimpeded water flow in river corridors by globally adjusting flowlines to rectify artificial obstructions from inconsistent DEM elevations or misalignments. Please read the documentation for more information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning river mesh\n",
    "#\n",
    "# adding elevations to the river tree for stream bed conditioning\n",
    "watershed_workflow.condition.setProfileByDEM(rivers, dem)\n",
    "\n",
    "# conditioning the river mesh using NHD elevations\n",
    "watershed_workflow.condition.conditionRiverMesh(m2, rivers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf32e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting surface mesh with elevations\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.inset_axes([0.65,0.05,0.3,0.5])\n",
    "cbax = fig.add_axes([0.05,0.05,0.9,0.05])\n",
    "\n",
    "mp = m2.plot(facecolors='elevation', edgecolors=None, ax=ax, linewidth=0.5)\n",
    "cbar = fig.colorbar(mp, orientation=\"horizontal\", cax=cbax)\n",
    "ax.set_title('surface mesh with elevations')\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "mp2 = m2.plot(facecolors='elevation', edgecolors='white', ax=ax2)\n",
    "ax2.set_aspect('equal', 'datalim')\n",
    "\n",
    "xlim = (1.4433e6, 1.4438e6)\n",
    "ylim = (-647000, -647500)\n",
    "\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax.indicate_inset_zoom(ax2, edgecolor='k')\n",
    "\n",
    "cbar.ax.set_title('elevation [m]')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets for subcatchments and outlets\n",
    "watershed_workflow.regions.addWatershedAndOutletRegions(m2, watershed, outlet_width=250, exterior_outlet=True)\n",
    "\n",
    "# add labeled sets for river corridor cells\n",
    "watershed_workflow.regions.addRiverCorridorRegions(m2, rivers)\n",
    "\n",
    "# add labeled sets for river corridor cells by order\n",
    "watershed_workflow.regions.addStreamOrderRegions(m2, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92519786",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4201fb",
   "metadata": {},
   "source": [
    "## Surface properties\n",
    "\n",
    "Meshes interact with data to provide forcing, parameters, and more in the actual simulation.  Specifically, we need vegetation type on the surface to provide information about transpiration and subsurface structure to provide information about water retention curves, etc.\n",
    "\n",
    "## NLCD for LULC\n",
    "\n",
    "We'll start by downloading and collecting land cover from the NLCD dataset, and generate sets for each land cover type that cover the surface.  Likely these will be some combination of grass, deciduous forest, coniferous forest, and mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NLCD raster\n",
    "nlcd = sources['land cover'].getDataset(watershed.exterior.buffer(100), watershed.crs, band=1)\n",
    "\n",
    "# what land cover types did we get?\n",
    "logging.info('Found land cover dtypes: {}'.format(nlcd.dtype))\n",
    "logging.info('Found land cover types: {}'.format(set(list(nlcd.values.ravel()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a colormap for the data\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "      watershed_workflow.colors.createNLCDColormap(np.unique(nlcd))\n",
    "nlcd_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "nlcd.plot.imshow(ax=ax, cmap=nlcd_cmap, norm=nlcd_norm, add_colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nlcd_indices), \n",
    "                               cmap=nlcd_cmap, labels=nlcd_labels, ax=ax) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map nlcd onto the mesh\n",
    "m2_nlcd = watershed_workflow.getDatasetOnMesh(m2, nlcd, method='nearest')\n",
    "m2.cell_data = pd.DataFrame({'land_cover': m2_nlcd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = m2.plot(facecolors=m2_nlcd, cmap=nlcd_cmap, norm=nlcd_norm, edgecolors=None, add_colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nlcd_indices), \n",
    "                               cmap=nlcd_cmap, labels=nlcd_labels, ax=plt.gca()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets to the mesh for NLCD\n",
    "nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "watershed_workflow.regions.addSurfaceRegions(m2, names=nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44450387",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65808d49",
   "metadata": {},
   "source": [
    "## MODIS LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ade3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = start #f'{start_year}-01-01'\n",
    "enddate = end #f'{end_year+1}-01-01'\n",
    "res = sources['LAI'].getDataset(watershed.exterior.bounds, crs, startdate, enddate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(res, watershed_workflow.sources.manager_modis_appeears.Task):\n",
    "    assert sources['LAI'].is_ready(res)\n",
    "    modis_data = sources['LAI'].getDataset(task=res)\n",
    "else:\n",
    "    modis_data = res\n",
    "\n",
    "assert modis_data['LAI'].rio.crs is not None\n",
    "print(modis_data['LULC'].rio.crs, modis_data['LULC'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55704322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap day (366th day of any leap year)\n",
    "for k,v in modis_data.items():\n",
    "    assert v.rio.crs is not None\n",
    "    new_v = watershed_workflow.data.filterLeapDay(v)\n",
    "    assert new_v.rio.crs is not None\n",
    "    modis_data[k] = new_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74288471-932d-46ed-9bdc-60ced1eb1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_data['LULC'][0].plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690701e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need new start and end dates in the new calendar\n",
    "startdate = start \n",
    "enddate = end \n",
    "\n",
    "# compute the dynamic time series\n",
    "lai_df = watershed_workflow.land_cover_properties.computeTimeSeries(modis_data['LAI'], modis_data['LULC'], \n",
    "                                                                      polygon=watershed.exterior, polygon_crs=watershed.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c647d-5582-4448-9ad4-53c68de2620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['modis_lai_dynamic'] = f'{name}_LAI_MODIS_Dynamic.h5'\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(outputs['modis_lai_dynamic'], lai_df)\n",
    "watershed_workflow.land_cover_properties.plotLAI(lai_df, indices='MODIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_smoothed = watershed_workflow.data.smoothTimeSeries(lai_df, 'time [datetime]')\n",
    "lai_typical_df = watershed_workflow.data.computeAverageYear(lai_smoothed, 'time [datetime]', output_nyears=10, \n",
    "                                                                  start_year=2000)\n",
    "\n",
    "\n",
    "outputs['modis_lai_cyclic_steadystate'] = f'{name}_LAI_MODIS_CyclicSteadystate.h5'\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(outputs['modis_lai_cyclic_steadystate'], lai_typical_df)\n",
    "watershed_workflow.land_cover_properties.plotLAI(lai_typical_df, indices='MODIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d0193",
   "metadata": {},
   "source": [
    "## Crosswalk of LAI to NLCD LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_lulc_mode = watershed_workflow.data.computeMode(modis_data['LULC'])\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "modis_lulc_mode.plot.imshow(ax=ax[0])\n",
    "modis_data['LULC'][0].plot.imshow(ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(watershed_workflow.crs.from_xarray(modis_data['LULC']))\n",
    "print(watershed_workflow.crs.from_xarray(modis_lulc_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk = watershed_workflow.land_cover_properties.computeCrosswalk(modis_lulc_mode, nlcd, method='fractional area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the NLCD-based time series\n",
    "nlcd_lai_cyclic_steadystate = watershed_workflow.land_cover_properties.applyCrosswalk(crosswalk, lai_typical_df)\n",
    "nlcd_lai_dynamic = watershed_workflow.land_cover_properties.applyCrosswalk(crosswalk, lai_df)\n",
    "\n",
    "watershed_workflow.land_cover_properties.removeNullLAI(nlcd_lai_cyclic_steadystate)\n",
    "watershed_workflow.land_cover_properties.removeNullLAI(nlcd_lai_dynamic)\n",
    "nlcd_lai_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d48c6-2172-4fb8-83d8-78de1ff7db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the NLCD-based time series to disk\n",
    "outputs['nlcd_lai_cyclic_steadystate'] = f'{name}_LAI_NLCD_CyclicSteadystate.h5'\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(outputs['nlcd_lai_cyclic_steadystate'], nlcd_lai_cyclic_steadystate)\n",
    "\n",
    "outputs['nlcd_lai_dynamic'] = f'{name}_LAI_NLCD_Dynamic.h5'\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(outputs['nlcd_lai_dynamic'], nlcd_lai_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84cce1",
   "metadata": {},
   "source": [
    "# Subsurface Soil, Geologic Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c14b57",
   "metadata": {},
   "source": [
    "## NRCS Soils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NRCS shapes, on a reasonable crs\n",
    "nrcs = sources['soil structure'].getShapesByGeometry(watershed.exterior, watershed.crs).to_crs(crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7edb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c16985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean dataframe with just the data we will need for ATS\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(nrcs, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(nrcs, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    nrcs.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "nan_mask = nrcs.isna().any(axis=1)\n",
    "dropped_mukeys = nrcs.index[nan_mask]\n",
    "\n",
    "# Drop those rows\n",
    "nrcs = nrcs[~nan_mask]\n",
    "\n",
    "assert nrcs['porosity [-]'][:].min() >= min_porosity\n",
    "assert nrcs['permeability [m^2]'][:].max() <= max_permeability\n",
    "nrcs\n",
    "\n",
    "# check for nans\n",
    "nrcs.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c749240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the soil color of each cell of the mesh\n",
    "soil_color_mukey = watershed_workflow.getShapePropertiesOnMesh(m2, nrcs, 'ID', \n",
    "                                                         resolution=50, nodata=-999)\n",
    "\n",
    "nrcs.set_index('ID', drop=False, inplace=True)\n",
    "\n",
    "unique_soil_colors = list(np.unique(soil_color_mukey))\n",
    "if -999 in unique_soil_colors:\n",
    "    unique_soil_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of soil_color\n",
    "nrcs = nrcs.loc[unique_soil_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "nrcs['ATS ID'] = range(1000, 1000+len(unique_soil_colors))\n",
    "nrcs.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new soil color and a soil thickness map using the ATS IDs\n",
    "soil_color = -np.ones_like(soil_color_mukey)\n",
    "soil_thickness = np.nan * np.ones(soil_color.shape, 'd')\n",
    "\n",
    "for ats_ID, ID, thickness in zip(nrcs.index, nrcs.ID, nrcs['thickness [m]']):\n",
    "    mask = np.where(soil_color_mukey == ID)\n",
    "    soil_thickness[mask] = thickness\n",
    "    soil_color[mask] = ats_ID\n",
    "\n",
    "m2.cell_data['soil_color'] = soil_color\n",
    "m2.cell_data['soil thickness'] = soil_thickness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the soil color\n",
    "# -- get a cmap for soil color\n",
    "sc_indices, sc_cmap, sc_norm, sc_ticks, sc_labels = \\\n",
    "      watershed_workflow.colors.createIndexedColormap(nrcs.index)\n",
    "\n",
    "mp = m2.plot(facecolors=m2.cell_data['soil_color'], cmap=sc_cmap, norm=sc_norm, edgecolors=None, add_colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nrcs), \n",
    "                               cmap=sc_cmap, labels=sc_labels, ax=plt.gca()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e407cec",
   "metadata": {},
   "source": [
    "## Depth to Bedrock from SoilGrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee52368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb = sources['depth to bedrock'].getDataset(watershed.exterior, watershed.crs)\n",
    "\n",
    "# the SoilGrids dataset is in cm --> convert to meters\n",
    "dtb.values = dtb.values/100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to the mesh\n",
    "m2.cell_data['dtb'] = watershed_workflow.getDatasetOnMesh(m2, dtb, method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gons = m2.plot(facecolors=m2.cell_data['dtb'], cmap='RdBu', edgecolors=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710933c",
   "metadata": {},
   "source": [
    "## GLHYMPs Geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d64c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps = sources['geologic structure'].getShapesByGeometry(watershed.exterior.buffer(1000), watershed.crs, \n",
    "                                              min_porosity=min_porosity, \n",
    "                                              max_permeability=max_permeability, \n",
    "                                              max_vg_alpha=max_vg_alpha)\n",
    "\n",
    "# convert to the right geometry (this should be done in a call to ww.getShapesByGeometry()!\n",
    "glhymps = glhymps.to_crs(watershed.crs)\n",
    "\n",
    "# intersect with the buffered geometry -- don't keep extras\n",
    "glhymps = glhymps[glhymps.intersects(watershed.exterior.buffer(10))]\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1942f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality check -- make sure glymps shapes cover the watershed\n",
    "print(glhymps.union_all().contains(watershed.exterior))\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ad78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "glhymps.pop('logk_stdev [-]')\n",
    "\n",
    "assert glhymps['porosity [-]'][:].min() >= min_porosity\n",
    "assert glhymps['permeability [m^2]'][:].max() <= max_permeability\n",
    "assert glhymps['van Genuchten alpha [Pa^-1]'][:].max() <= max_vg_alpha\n",
    "\n",
    "glhymps.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for larger areas there are often common regions -- two labels with the same properties -- no need to duplicate those with identical values.\n",
    "def reindex_remove_duplicates(df, index):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(glhymps, 'ID')\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the geo color of each cell of the mesh\n",
    "geology_color_glhymps = watershed_workflow.getShapePropertiesOnMesh(m2, glhymps, 'index', \n",
    "                                                         resolution=50, nodata=-999)\n",
    "\n",
    "# retain only the unique values of geology that actually appear in our cell mesh\n",
    "unique_geology_colors = list(np.unique(geology_color_glhymps))\n",
    "if -999 in unique_geology_colors:\n",
    "    unique_geology_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of geology_color\n",
    "glhymps = glhymps.loc[unique_geology_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "glhymps['ATS ID'] = range(100, 100+len(unique_geology_colors))\n",
    "glhymps['TMP_ID'] = glhymps.index\n",
    "glhymps.reset_index(drop=True, inplace=True)\n",
    "glhymps.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new geology color using the ATS IDs\n",
    "geology_color = -np.ones_like(geology_color_glhymps)\n",
    "for ats_ID, tmp_ID in zip(glhymps.index, glhymps.TMP_ID):\n",
    "    geology_color[np.where(geology_color_glhymps == tmp_ID)] = ats_ID\n",
    "\n",
    "glhymps.pop('TMP_ID')\n",
    "\n",
    "m2.cell_data['geology_color'] = geology_color\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geology_color_glhymps.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280a14a",
   "metadata": {},
   "source": [
    "## Combine to form a complete subsurface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = watershed_workflow.soil_properties.getDefaultBedrockProperties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pd.concat([glhymps, nrcs, bedrock])\n",
    "\n",
    "# save the properties to disk for use in generating input file\n",
    "outputs['subsurface_properties_filename'] = os.path.join(f'{name}_subsurface_properties.csv')\n",
    "subsurface_props.to_csv(outputs['subsurface_properties_filename'])\n",
    "subsurface_props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ca244",
   "metadata": {},
   "source": [
    "# Extrude the 2D Mesh to make a 3D mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the floor of the domain as max DTB\n",
    "dtb_max = np.nanmax(m2.cell_data['dtb'].values)\n",
    "m2.cell_data['dtb'] = m2.cell_data['dtb'].fillna(dtb_max)\n",
    "\n",
    "print(f'total thickness: {dtb_max} m')\n",
    "total_thickness = 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49322734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "#\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = watershed_workflow.mesh.optimizeDzs(0.05, 0.5, 2, 10)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50m total thickness, minus 2m soil thickness, leaves us with 48 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = watershed_workflow.mesh.optimizeDzs(1, 10, 48, 8)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "dzs_geo = [1.0, 2.0, 4.0, 8.0, 11, 11, 11]\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer extrusion\n",
    "DTB = m2.cell_data['dtb'].values\n",
    "soil_color = m2.cell_data['soil_color'].values\n",
    "geo_color = m2.cell_data['geology_color'].values\n",
    "soil_thickness = m2.cell_data['soil thickness'].values\n",
    "\n",
    "\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    # use glhymps params\n",
    "    br_or_geo = np.where(depth < DTB, geo_color, 999)\n",
    "    soil_or_br_or_geo = np.where(np.bitwise_and(soil_color > 0, depth < soil_thickness),\n",
    "                                 soil_color,\n",
    "                                 br_or_geo)\n",
    "\n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    geo_or_br = np.where(depth < DTB, geo_color, 999)\n",
    "\n",
    "    layer_mat_ids.append(geo_or_br)\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "watershed_workflow.mesh.Mesh3D.summarizeExtrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)\n",
    "\n",
    "# downselect subsurface properties to only those that are used\n",
    "layer_mat_id_used = list(np.unique(np.array(layer_mat_ids)))\n",
    "subsurface_props_used = subsurface_props.loc[layer_mat_id_used]\n",
    "subsurface_props_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = watershed_workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45294ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2D labeled sets')\n",
    "print('---------------')\n",
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D labeled sets')\n",
    "print('------------------------')\n",
    "for ls in m3.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D side sets')\n",
    "print('---------------------')\n",
    "for ls in m3.side_sets:\n",
    "    print(f'{ls.setid} : FACE : {len(ls.cell_list)} : \"{ls.name}\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mesh to disk\n",
    "outputs['mesh_filename'] = os.path.join('output_data',f'{name}.exo')\n",
    "try:\n",
    "    os.remove(outputs['mesh_filename'])\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "m3.writeExodus(outputs['mesh_filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf97310",
   "metadata": {},
   "source": [
    "## Meteorological forcing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edb301",
   "metadata": {},
   "source": [
    "In addition to the LAI covered above, ATS is driven by meteorological forcing data. We need two forcing datasets -- the actual time series and a typical year for use in spinup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed657034",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = cftime.datetime(start.year, start.month, start.day, calendar='noleap')\n",
    "enddate = cftime.datetime(end.year, end.month, end.day, calendar='noleap')\n",
    "\n",
    "# download the data\n",
    "met_data_raw = sources['meteorology'].getDataset(watershed.exterior, crs, startdate, enddate)\n",
    "\n",
    "# write raw data to HDF5\n",
    "filename = os.path.join('output_data', f'{name}_daymet-raw.h5')\n",
    "outputs['meteorology_raw'] = filename\n",
    "watershed_workflow.io.writeDatasetToHDF5(\n",
    "    filename,\n",
    "    met_data_raw, \n",
    "    watershed_workflow.daymet.getAttributes(watershed.exterior.bounds, startdate, enddate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9e372-51a2-40fb-bcdb-765a9c6705bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and write ATS format for transient run\n",
    "met_data_transient = watershed_workflow.daymet.convertToATS(met_data_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a7af0-a47f-4c7a-926c-a0cb103523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few of the met data -- does it look reasonable?\n",
    "date = np.array([datetime.datetime(t.year, t.month, t.day) for t in met_data_transient.time.values])\n",
    "            \n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].plot(date, met_data_transient['precipitation rain [m s^-1]'].data[:,5,5])\n",
    "axs[0].set_ylabel('rain [m d^-1]')\n",
    "\n",
    "axs[1].plot(date, met_data_transient['incoming shortwave radiation [W m^-2]'].data[:,5,5], 'r')\n",
    "axs[1].set_ylabel('incoming shortwave radiation [W m^-2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ed0a9-3b3b-4b3f-be45-96004e5bd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write transient data to disk\n",
    "filename = os.path.join('output_data', f'{name}_daymet-{start.year}-{end.year}.h5')\n",
    "outputs['transient meteorology'] = filename\n",
    "watershed_workflow.io.writeDatasetToHDF5(\n",
    "    filename,\n",
    "    met_data_transient,\n",
    "    watershed_workflow.daymet.getAttributes(watershed.exterior.bounds, startdate, enddate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ec1e2-8a12-4117-9821-21715d79d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the typical year of the _raw_ data\n",
    "# note that we set interpolate to False, since met_data is already daily on a noleap calendar\n",
    "met_data_raw_avg = watershed_workflow.data.computeAverageYear(met_data_raw, time_column = 'time', start_year = start.year, output_nyears=nyears_cyclic_steadystate)\n",
    "\n",
    "# smoothing precipitation doesn't work -- even averaging precip doesn't work because both result in more events of lower\n",
    "# intensity.  Instead, we pick the median year and repeat that nyears_cyclic_steadystate times\n",
    "precip_raw = met_data_raw['prcp'].data\n",
    "shape_xy = precip_raw.shape[1:]\n",
    "precip_raw = precip_raw.reshape((-1, 365,)+shape_xy)\n",
    "annual_precip_raw = precip_raw.sum(axis=(1,2,3))\n",
    "\n",
    "# note -- don't use np.median here... for even number of years it will not appear.  Instead, sort and talk the halfway point\n",
    "median_i = sorted(((i,v) for (i,v) in enumerate(annual_precip_raw)), key=lambda x : x[1])[len(annual_precip_raw)//2][0]\n",
    "typical_precip_raw = precip_raw[median_i]\n",
    "met_data_raw_avg['prcp'] = (('time', 'y', 'x'), np.tile(typical_precip_raw, (nyears_cyclic_steadystate,1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc9e7e-7c14-441b-88aa-b25547d2875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the typical year to ATS\n",
    "met_data_avg = watershed_workflow.daymet.convertToATS(met_data_raw_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52805c2-5256-413f-8fc9-d28da9dc7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few of the met data -- does it look reasonable?\n",
    "fig, axs = plt.subplots(1,2)\n",
    "date = np.array([datetime.datetime(t.year, t.month, t.day) for t in met_data_avg.time.values])\n",
    "\n",
    "axs[0].plot(date, met_data_avg['precipitation rain [m s^-1]'].data[:,5,5], label='rain')\n",
    "axs[0].plot(date, met_data_avg['precipitation snow [m SWE s^-1]'].data[:,5,5], label='snow')\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel('precip [m d^-1]')\n",
    "\n",
    "axs[1].plot(date, met_data_avg['incoming shortwave radiation [W m^-2]'].data[:,5,5], 'r')\n",
    "axs[1].set_ylabel('incoming shortwave radiation [W m^-2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638c44f-ddde-4b13-9040-dcf438497ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cyclic steadystate data to disk\n",
    "filename = os.path.join('output_data', f'{name}_daymet-typical.h5')\n",
    "outputs['cyclic steadystate meteorology'] = filename\n",
    "watershed_workflow.io.writeDatasetToHDF5(\n",
    "    filename,\n",
    "    met_data_avg,\n",
    "    watershed_workflow.daymet.getAttributes(watershed.exterior.bounds, startdate, enddate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580a4d9-fb76-43d0-a17d-176d77938db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average precip rate for steadystate solution\n",
    "precip_mean = (met_data_transient['precipitation rain [m s^-1]'].data + met_data_transient['precipitation snow [m SWE s^-1]'].data).mean()\n",
    "logging.info(f'Mean precip value = {precip_mean}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d93651-1885-40de-b7d4-49448ffc2756",
   "metadata": {},
   "source": [
    "## Write ATS input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad766d0c",
   "metadata": {
    "papermill": {
     "duration": 1.143055,
     "end_time": "2022-03-07T15:59:14.911795",
     "exception": false,
     "start_time": "2022-03-07T15:59:13.768740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that each of these are defined as functions so we can reuse them for all three input files.\n",
    "\n",
    "# add the subsurface and surface domains\n",
    "#\n",
    "# Note this also adds a \"computational domain\" region to the region list, and a vis spec \n",
    "# for \"domain\"\n",
    "def add_domains(main_list, mesh_filename, surface_region='surface', snow=True, canopy=True):\n",
    "    ats_input_spec.public.add_domain(main_list, \n",
    "                                 domain_name='domain', \n",
    "                                 dimension=3, \n",
    "                                 mesh_type='read mesh file',\n",
    "                                 mesh_args={'file':mesh_filename})\n",
    "    if surface_region:\n",
    "        main_list['mesh']['domain']['build columns from set'] = surface_region    \n",
    "    \n",
    "        # Note this also adds a \"surface domain\" region to the region list and a vis spec for \n",
    "        # \"surface\"\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='surface',\n",
    "                                dimension=2,\n",
    "                                mesh_type='surface',\n",
    "                                mesh_args={'surface sideset name':'surface'})\n",
    "    if snow:\n",
    "        # Add the snow and canopy domains, which are aliases to the surface\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='snow',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n",
    "    if canopy:\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='canopy',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace57f3",
   "metadata": {
    "papermill": {
     "duration": 1.135431,
     "end_time": "2022-03-07T15:59:17.151867",
     "exception": false,
     "start_time": "2022-03-07T15:59:16.016436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_land_cover(main_list):\n",
    "    # next write a land-cover section for each NLCD type\n",
    "    for index, nlcd_name in zip(nlcd_indices, nlcd_labels):\n",
    "        ats_input_spec.public.set_land_cover_default_constants(main_list, nlcd_name)\n",
    "\n",
    "    land_cover_list = main_list['state']['initial conditions']['land cover types']\n",
    "    # update some defaults for\n",
    "    # ['Other', 'Deciduous Forest']\n",
    "    # note, these are from the CLM Technical Note v4.5\n",
    "    #\n",
    "    # Rooting depth curves from CLM TN 4.5 table 8.3\n",
    "    #\n",
    "    # Note, the mafic potential values are likely pretty bad for the types of van Genuchten \n",
    "    # curves we are using (ETC -- add paper citation about this topic).  Likely they need\n",
    "    # to be modified.  Note that these values are in [mm] from CLM TN 4.5 table 8.1, so the \n",
    "    # factor of 10 converts to [Pa]\n",
    "    #\n",
    "    # Note, albedo of canopy taken from CLM TN 4.5 table 3.1\n",
    "    land_cover_list['Deciduous Forest']['rooting profile alpha [-]'] = 6.0\n",
    "    land_cover_list['Deciduous Forest']['rooting profile beta [-]'] = 2.0\n",
    "    land_cover_list['Deciduous Forest']['rooting depth max [m]'] = 10.0\n",
    "    land_cover_list['Deciduous Forest']['capillary pressure at fully closed stomata [Pa]'] = 224000\n",
    "    land_cover_list['Deciduous Forest']['capillary pressure at fully open stomata [Pa]'] = 35000 * .10\n",
    "    land_cover_list['Deciduous Forest']['albedo of canopy [-]'] = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede914b",
   "metadata": {
    "papermill": {
     "duration": 1.421863,
     "end_time": "2022-03-07T15:59:19.669461",
     "exception": false,
     "start_time": "2022-03-07T15:59:18.247598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add soil sets: note we need a way to name the set, so we use, e.g. SSURGO-MUKEY.\n",
    "def soil_set_name(ats_id):\n",
    "    return subsurface_props_used.loc[ats_id, 'name']\n",
    "\n",
    "def add_soil_properties(main_list):\n",
    "    # add soil material ID regions, porosity, permeability, and WRMs\n",
    "    for ats_id in subsurface_props_used.index:\n",
    "        props = subsurface_props_used.loc[ats_id]\n",
    "        set_name = soil_set_name(ats_id)\n",
    "        \n",
    "        if props['van Genuchten n [-]'] < 1.5:\n",
    "            smoothing_interval = 0.01\n",
    "        else:\n",
    "            smoothing_interval = 0.0\n",
    "        \n",
    "        ats_input_spec.public.add_soil_type(main_list, set_name, ats_id, outputs['mesh_filename'],\n",
    "                                            float(props['porosity [-]']),\n",
    "                                            float(props['permeability [m^2]']), 1.e-7,\n",
    "                                            float(props['van Genuchten alpha [Pa^-1]']),\n",
    "                                            float(props['van Genuchten n [-]']),\n",
    "                                            float(props['residual saturation [-]']),\n",
    "                                            float(smoothing_interval))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28898b6-b588-4755-94c9-b201c08152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an ATS \"main\" input spec list -- note, this is a dummy and is not used to write any files yet\n",
    "def get_main():\n",
    "    main_list = ats_input_spec.public.get_main()\n",
    "\n",
    "    # add the mesh and all domains\n",
    "    mesh_filename = os.path.join('..', outputs['mesh_filename'])\n",
    "    add_domains(main_list, mesh_filename)\n",
    "\n",
    "    # add labeled sets\n",
    "    for ls in m3.labeled_sets:\n",
    "        ats_input_spec.public.add_region_labeled_set(main_list, ls.name, ls.setid, mesh_filename, ls.entity)\n",
    "    for ss in m3.side_sets:\n",
    "        ats_input_spec.public.add_region_labeled_set(main_list, ss.name, ss.setid, mesh_filename, 'FACE')\n",
    "    \n",
    "    # add land cover\n",
    "    add_land_cover(main_list)\n",
    "\n",
    "    # add soil properties\n",
    "    add_soil_properties(main_list)\n",
    "        \n",
    "    # add observations for each subcatchment\n",
    "    ats_input_spec.public.add_observations_water_balance(main_list, \"computational domain\", \n",
    "                                                         \"surface domain\", \"external_sides\")\n",
    "\n",
    "    return main_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d6e87",
   "metadata": {
    "papermill": {
     "duration": 1.152786,
     "end_time": "2022-03-07T15:59:21.928795",
     "exception": false,
     "start_time": "2022-03-07T15:59:20.776009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_basic_properties(xml, main_xml):\n",
    "    \"\"\"This function updates an xml object with the above properties for mesh, regions, soil props, and lc props\"\"\"\n",
    "    # find and replace the mesh list\n",
    "    xml.replace('mesh', asearch.child_by_name(main_xml, 'mesh'))\n",
    "\n",
    "    # find and replace the regions list\n",
    "    xml.replace('regions', asearch.child_by_name(main_xml, 'regions'))\n",
    "    \n",
    "    # update all model parameters lists\n",
    "    xml_parlist = asearch.find_path(xml, ['state', 'model parameters'], no_skip=True)\n",
    "    for parlist in asearch.find_path(main_xml, ['state', 'model parameters'], no_skip=True):\n",
    "        try:\n",
    "            xml_parlist.replace(parlist.getName(), parlist)\n",
    "        except aerrors.MissingXMLError:\n",
    "            xml_parlist.append(parlist)\n",
    "\n",
    "    # update all evaluator lists\n",
    "    xml_elist = asearch.find_path(xml, ['state', 'evaluators'], no_skip=True)\n",
    "    for elist in asearch.find_path(main_xml, ['state', 'evaluators'], no_skip=True):\n",
    "        try:\n",
    "            xml_elist.replace(elist.getName(), elist)\n",
    "        except aerrors.MissingXMLError:\n",
    "            xml_elist.append(elist)    \n",
    "    \n",
    "    # find and replace land cover\n",
    "    consts_list = asearch.find_path(xml, ['state', 'initial conditions'])\n",
    "    lc_list = asearch.find_path(main_xml, ['state', 'initial conditions', 'land cover types'], no_skip=True)\n",
    "    \n",
    "    try:\n",
    "        consts_list.replace('land cover types', lc_list)\n",
    "    except aerrors.MissingXMLError:\n",
    "        consts_list.append(lc_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374e7a7",
   "metadata": {
    "papermill": {
     "duration": 1.076236,
     "end_time": "2022-03-07T15:59:24.162869",
     "exception": false,
     "start_time": "2022-03-07T15:59:23.086633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the first file, we load a spinup template and write the needed quantities into that file, saving it to the appropriate run directory.  Note there is no DayMet or land cover or LAI properties needed for this run.  The only property that is needed is the domain-averaged, mean annual rainfall rate.  We then take off some for ET (note too wet spins up faster than too dry, so don't take off too much...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6d63a",
   "metadata": {
    "papermill": {
     "duration": 1.368452,
     "end_time": "2022-03-07T16:00:03.262110",
     "exception": false,
     "start_time": "2022-03-07T16:00:01.893658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_spinup_steadystate(name, precip_mean, **kwargs):\n",
    "    # create the main list\n",
    "    main = get_main()\n",
    "\n",
    "    # set precip to 0.6 * the mean precip value\n",
    "    precip = main['state']['evaluators'].append_empty('surface-precipitation')\n",
    "    precip.set_type('independent variable constant', ats_input_spec.public.known_specs['evaluator-independent-variable-constant-spec'])\n",
    "    precip['value'] = float(precip_mean * .6)\n",
    "\n",
    "    \n",
    "    # load the template file\n",
    "    prefix = 'steadystate'\n",
    "    xml = aio.fromFile(os.path.join('input_data', f'{prefix}-template.xml'))\n",
    "    \n",
    "    # update the template xml with the main xml generated here\n",
    "    main_xml = ats_input_spec.io.to_xml(main)\n",
    "    populate_basic_properties(xml, main_xml, **kwargs)\n",
    "\n",
    "    # write to disk\n",
    "    outputs[f'{prefix}_filename'] = f'{name}-{prefix}.xml'\n",
    "    filename = outputs[f'{prefix}_filename']\n",
    "    aio.toFile(xml, filename)\n",
    "\n",
    "    # create a run directory\n",
    "    outputs[f'{prefix}_rundir'] = f'{name}-{prefix}'\n",
    "    rundir = outputs[f'{prefix}_rundir']\n",
    "    os.makedirs(rundir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5a99",
   "metadata": {
    "papermill": {
     "duration": 1.115332,
     "end_time": "2022-03-07T16:00:05.524409",
     "exception": false,
     "start_time": "2022-03-07T16:00:04.409077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the second file, we load a transient run template.  This file needs the basics, plus DayMet and LAI as the \"typical year data\".  Also we set the run directory that will be used for the steadystate run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d6e3c",
   "metadata": {
    "papermill": {
     "duration": 1.116433,
     "end_time": "2022-03-07T16:00:07.782884",
     "exception": false,
     "start_time": "2022-03-07T16:00:06.666451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the third file, we load a transient run template as well.  This file needs the basics, DayMet with the actual data, and we choose for this run to use the MODIS typical year.  MODIS is only available for 2002 on, so if we didn't need 1980-2002 we could use the real data, but for this run we want a longer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b76cb6-096c-4673-b32f-85843e12ac46",
   "metadata": {
    "papermill": {
     "duration": 1.155846,
     "end_time": "2022-03-07T16:00:10.035975",
     "exception": false,
     "start_time": "2022-03-07T16:00:08.880129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_transient(name, cyclic_steadystate=False, **kwargs):\n",
    "    # make a unique name based on options\n",
    "    logging.info(f'Writing transient: {name}')\n",
    "\n",
    "    if cyclic_steadystate:\n",
    "        prefix = 'cyclic_steadystate'\n",
    "        previous = 'steadystate'\n",
    "    else:\n",
    "        prefix = 'transient'\n",
    "        previous = 'cyclic_steadystate'\n",
    "\n",
    "    main = get_main()\n",
    "\n",
    "    # add the DayMet evaluators\n",
    "    if cyclic_steadystate:\n",
    "        daymet_filename = outputs['cyclic steadystate meteorology']\n",
    "    else:\n",
    "        daymet_filename = outputs['transient meteorology']\n",
    "    ats_input_spec.public.add_daymet_box_evaluators(main, os.path.join('..', daymet_filename), True)\n",
    "\n",
    "    # add the LAI filenames\n",
    "    if cyclic_steadystate:\n",
    "        lai_filename = outputs['nlcd_lai_cyclic_steadystate']\n",
    "    else:\n",
    "        lai_filename = outputs['nlcd_lai_dynamic']\n",
    "    ats_input_spec.public.add_lai_point_evaluators(main, os.path.join('..', lai_filename), list(nlcd_labels_dict.values()))\n",
    "    \n",
    "    # load the template file\n",
    "    template_filename = os.path.join('input_data', f'{prefix}-template.xml')\n",
    "    xml = aio.fromFile(template_filename)\n",
    "\n",
    "    # update the template xml with the main xml generated here\n",
    "    main_xml = ats_input_spec.io.to_xml(main)\n",
    "    populate_basic_properties(xml, main_xml, **kwargs)\n",
    "    \n",
    "    # update the start and end time -- would be nice to set these in main, but it would be \n",
    "    # confusing as to when to copy them in populate_basic_properties and when not to do so.\n",
    "    start_day = 274\n",
    "    if cyclic_steadystate:\n",
    "        end_day = 274 + (nyears_cyclic_steadystate - 1) * 365 \n",
    "    else:\n",
    "        end_day = 274 + (enddate - startdate).days \n",
    "        \n",
    "    par = asearch.find_path(xml, ['cycle driver', 'start time'])\n",
    "    par.setValue(start_day)\n",
    "\n",
    "    par = asearch.find_path(xml, ['cycle driver', 'end time'])\n",
    "    par.setValue(end_day)\n",
    "    \n",
    "    # update the restart filenames\n",
    "    for var in asearch.findall_path(xml, ['initial condition', 'restart file']):\n",
    "        var.setValue(os.path.join('..', outputs[f'{previous}_rundir'], 'checkpoint_final.h5'))\n",
    "\n",
    "    # update the observations list\n",
    "    obs = next(i for (i,el) in enumerate(xml) if el.get('name') == 'observations')\n",
    "    xml[obs] = asearch.child_by_name(main_xml, 'observations')\n",
    "   \n",
    "    # write to disk and make a directory for running the run\n",
    "    outputs[f'{prefix}_filename'] = f'{name}-{prefix}.xml'\n",
    "    filename = outputs[f'{prefix}_filename']\n",
    "\n",
    "    outputs[f'{prefix}_rundir'] = os.path.join(f'{name}-{prefix}')\n",
    "    rundir = outputs[f'{prefix}_rundir']\n",
    "\n",
    "    aio.toFile(xml, filename)\n",
    "    os.makedirs(rundir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf75414-0ed3-4257-b3af-621ff5853904",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_spinup_steadystate(name, precip_mean)\n",
    "write_transient(name, True)\n",
    "write_transient(name, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d3cb8",
   "metadata": {
    "papermill": {
     "duration": 1.175505,
     "end_time": "2022-03-07T16:00:14.959351",
     "exception": false,
     "start_time": "2022-03-07T16:00:13.783846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info('this workflow is a total success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765b51b-2a5a-444c-8632-10c316a03491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:ww-geopandas-20250725]",
   "language": "python",
   "name": "conda-env-ww-geopandas-20250725-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.28134,
   "end_time": "2022-03-07T16:00:23.770205",
   "environment_variables": {},
   "exception": true,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.005
   },
   "start_time": "2022-03-07T15:48:58.488865",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
